<?xml version="1.0" encoding="UTF-8"?>
<cronentries>

  <!--
    /cron/fanout params:
        queue=<QUEUE_NAME>
        endpoint=<ENDPOINT_NAME> // URL Path of servlet, which may contain placeholders:
        runInEmpty               // Run once, with no tld parameter
        forEachRealTld           // Run for tlds with getTldType() == TldType.REAL
        forEachTestTld           // Run for tlds with getTldType() == TldType.TEST
        exclude=TLD1[,TLD2]      // exclude something otherwise included
   -->

  <cron>
    <url>/_dr/task/rdeStaging</url>
    <description>
      This job generates a full RDE escrow deposit as a single gigantic XML document
      and streams it to cloud storage. When this job has finished successfully, it'll
      launch a separate task that uploads the deposit file to Iron Mountain via SFTP.
    </description>
    <!--
      This only needs to run once per day, but we launch additional jobs in case the
      cursor is lagging behind, so it'll catch up to the current date as quickly as
      possible. The only job that'll run under normal circumstances is the one that's
      close to midnight, since if the cursor is up-to-date, the task is a no-op.

      We want it to be close to midnight because that reduces the chance that the
      point-in-time code won't have to go to the extra trouble of fetching old
      versions of objects from Datastore. However, we don't want it to run too
      close to midnight, because there's always a chance that a change which was
      timestamped before midnight hasn't fully been committed to Datastore. So
      we add a 4+ minute grace period to ensure the transactions cool down, since
      our queries are not transactional.
    -->
    <schedule>every 8 hours from 00:07 to 20:00</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=rde-upload&endpoint=/_dr/task/rdeUpload&forEachRealTld]]></url>
    <description>
      This job is a no-op unless RdeUploadCursor falls behind for some reason.
    </description>
    <schedule>every 4 hours synchronized</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=rde-report&endpoint=/_dr/task/rdeReport&forEachRealTld]]></url>
    <description>
      This job is a no-op unless RdeReportCursor falls behind for some reason.
    </description>
    <schedule>every 4 hours synchronized</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=marksdb&endpoint=/_dr/task/tmchDnl&runInEmpty]]></url>
    <description>
      This job downloads the latest DNL from MarksDB and inserts it into the database.
      (See: TmchDnlServlet, ClaimsList)
    </description>
    <schedule>every 12 hours synchronized</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=marksdb&endpoint=/_dr/task/tmchSmdrl&runInEmpty]]></url>
    <description>
      This job downloads the latest SMDRL from MarksDB and inserts it into the database.
      (See: TmchSmdrlServlet, SignedMarkRevocationList)
    </description>
    <schedule>every 12 hours from 00:15 to 12:15</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=marksdb&endpoint=/_dr/task/tmchCrl&runInEmpty]]></url>
    <description>
      This job downloads the latest CRL from MarksDB and inserts it into the database.
      (See: TmchCrlServlet)
    </description>
    <schedule>every 12 hours synchronized</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=retryable-cron-tasks&endpoint=/_dr/task/syncGroupMembers&runInEmpty]]></url>
    <description>
      Syncs RegistrarContact changes in the past hour to Google Groups.
    </description>
    <schedule>every 1 hours synchronized</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=sheet&endpoint=/_dr/task/syncRegistrarsSheet&runInEmpty]]></url>
    <description>
      Synchronize Registrar entities to Google Spreadsheets.
    </description>
    <schedule>every 1 hours synchronized</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/task/resaveAllEppResources?fast=true]]></url>
    <description>
      This job resaves all our resources, projected in time to "now".
      It is needed for "deleteOldCommitLogs" to work correctly.
    </description>
    <schedule>1st monday of month 09:00</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/task/updateRegistrarRdapBaseUrls]]></url>
    <description>
      This job reloads all registrar RDAP base URLs from ICANN.
    </description>
    <schedule>every day 02:34</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/task/deleteOldCommitLogs]]></url>
    <description>
      This job deletes unreferenced commit logs from Datastore that are older than thirty days.
      Since references are only updated on save, if we want to delete "unneeded" commit logs, we
      also need "resaveAllEppResources" to run periodically.
    </description>
    <schedule>3rd monday of month 09:00</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/commitLogCheckpoint]]></url>
    <description>
      This job checkpoints the commit log buckets and exports the diff since last checkpoint to GCS.
    </description>
    <schedule>every 3 minutes synchronized</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=retryable-cron-tasks&endpoint=/_dr/task/exportDomainLists&runInEmpty]]></url>
    <description>
      This job exports lists of all active domain names to Google Drive and Google Cloud Storage.
    </description>
    <schedule>every 12 hours synchronized</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/task/deleteContactsAndHosts]]></url>
    <description>
      This job runs a mapreduce that processes batch asynchronous deletions of
      contact and host resources by mapping over all EppResources and checking
      for any references to the contacts/hosts in pending deletion.
    </description>
    <schedule>every 5 minutes synchronized</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/task/refreshDnsOnHostRename]]></url>
    <description>
      This job runs a mapreduce that asynchronously handles DNS refreshes for
      host renames by mapping over all domains and creating DNS refresh tasks
      for any domains that reference a renamed host.
    </description>
    <schedule>every 5 minutes synchronized</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/task/expandRecurringBillingEvents]]></url>
    <description>
      This job runs a mapreduce that creates synthetic OneTime billing events from Recurring billing
      events. Events are created for all instances of Recurring billing events that should exist
      between the RECURRING_BILLING cursor's time and the execution time of the mapreduce.
    </description>
    <schedule>every day 03:00</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=export-snapshot&endpoint=/_dr/task/backupDatastore&runInEmpty]]></url>
    <description>
      This job fires off a Datastore managed-export job that generates snapshot files in GCS.
      It also enqueues a new task to wait on the completion of that job and then load the resulting
      snapshot into bigquery.
    </description>
    <!--
      Keep google.registry.export.CheckBackupAction.MAXIMUM_BACKUP_RUNNING_TIME less than
      this interval. -->
    <schedule>every day 06:00</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=nordn&endpoint=/_dr/task/nordnUpload&forEachRealTld&lordn-phase=sunrise]]></url>
    <description>
      This job uploads LORDN Sunrise CSV files for each TLD to MarksDB. It should be
      run at most every three hours, or at absolute minimum every 26 hours.
    </description>
    <!-- This may be set anywhere between "every 3 hours" and "every 25 hours". -->
    <schedule>every 12 hours synchronized</schedule>
    <timezone>UTC</timezone>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=nordn&endpoint=/_dr/task/nordnUpload&forEachRealTld&lordn-phase=claims]]></url>
    <description>
      This job uploads LORDN Claims CSV files for each TLD to MarksDB. It should be
      run at most every three hours, or at absolute minimum every 26 hours.
    </description>
    <!-- This may be set anywhere between "every 3 hours" and "every 25 hours". -->
    <schedule>every 12 hours synchronized</schedule>
    <timezone>UTC</timezone>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=retryable-cron-tasks&endpoint=/_dr/task/deleteProberData&runInEmpty]]></url>
    <description>
      This job clears out data from probers and runs once a week.
    </description>
    <schedule>every monday 14:00</schedule>
    <timezone>UTC</timezone>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=retryable-cron-tasks&endpoint=/_dr/task/exportReservedTerms&forEachRealTld]]></url>
    <description>
      Reserved terms export to Google Drive job for creating once-daily exports.
    </description>
    <schedule>every day 05:30</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=retryable-cron-tasks&endpoint=/_dr/task/exportPremiumTerms&forEachRealTld]]></url>
    <description>
      Exports premium price lists to the Google Drive folders for each TLD once per day.
    </description>
    <schedule>every day 05:00</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/readDnsQueue?jitterSeconds=45]]></url>
    <description>
      Lease all tasks from the dns-pull queue, group by TLD, and invoke PublishDnsUpdates for each
      group.
    </description>
    <schedule>every 1 minutes synchronized</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_ah/sessioncleanup?clear]]></url>
    <description>
      Delete up to 100 expired _ah_SESSION entities from Datastore.
    </description>
    <schedule>every 15 minutes</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=retryable-cron-tasks&endpoint=/_dr/task/icannReportingStaging&runInEmpty]]></url>
    <description>
      Create ICANN activity and transaction reports for last month, storing them in
      gs://[PROJECT-ID]-reporting/icann/monthly/yyyy-MM
      Upon success, enqueues the icannReportingUpload task to POST these files to ICANN.
    </description>
    <schedule>2 of month 09:00</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=retryable-cron-tasks&endpoint=/_dr/task/icannReportingUpload&runInEmpty]]></url>
    <description>
      Checks if the monthly ICANN reports have been successfully uploaded. If they have not, attempts to upload them again.
      Most of the time, this job should not do anything since the uploads are triggered when the reports are staged.
      However, in the event that an upload failed for any reason (e.g. ICANN server is down, IP allow list issues),
      this cron job will continue to retry uploads daily until they succeed.
    </description>
    <schedule>every day 15:00</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=retryable-cron-tasks&endpoint=/_dr/task/generateInvoices&runInEmpty]]></url>
    <description>
      Starts the beam/invoicing/InvoicingPipeline Dataflow template, which creates the overall invoice and
      detail report CSVs for last month, storing them in gs://[PROJECT-ID]-billing/invoices/yyyy-MM.
      Upon success, sends an e-mail copy of the invoice to billing personnel, and copies detail
      reports to the associated registrars' drive folders.
      See GenerateInvoicesAction for more details.
    </description>
    <!--WARNING: This must occur AFTER expandRecurringBillingEvents and AFTER exportSnapshot, as
    it uses Bigquery as the source of truth for billable events. ExportSnapshot usually takes
    about 2 hours to complete, so we give 11 hours to be safe. Normally, we give 24+ hours (see
    icannReportingStaging), but the invoicing team prefers receiving the e-mail on the first of
    each month. -->
    <schedule>1 of month 17:00</schedule>
    <target>backend</target>
  </cron>

  <cron>
    <url><![CDATA[/_dr/cron/fanout?queue=retryable-cron-tasks&endpoint=/_dr/task/generateSpec11&runInEmpty]]></url>
    <description>
      Starts the beam/spec11/Spec11Pipeline Dataflow template, which creates today's Spec11
      report. This report is stored in gs://[PROJECT-ID]-reporting/icann/spec11/yyyy-MM/.
      This job will only send email notifications on the second of every month.
      See GenerateSpec11ReportAction for more details.
    </description>
    <schedule>every day 15:00</schedule>
    <target>backend</target>
  </cron>
</cronentries>
